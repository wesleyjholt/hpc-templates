{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for running many small independent jobs in parallel\n",
    "\n",
    "Let's test it out. First, let's create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpkl\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m12345\u001b[39m)\n\u001b[1;32m      9\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Create some dummy data\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "N = 100\n",
    "d = 2\n",
    "data = list(rng.normal(size=(N, d)))\n",
    "\n",
    "with open('hpc/run_1/data.pkl', 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "\n",
    "# with open('hpc/run_1/data.txt', 'w') as f:\n",
    "#     for x in data:\n",
    "#         f.write(' '.join(map(str, x)) + '\\n')\n",
    "pd.DataFrame(data).to_csv('hpc/run_1/data.csv', header=False, index=False)\n",
    "\n",
    "# read data\n",
    "with open('hpc/run_1/data.txt', 'r') as f:\n",
    "    data = [list(map(float, line.strip().split())) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follow the steps in `README.md` to push the files to the cluster, run computations, and pull the results back to the local machine.\n",
    "\n",
    "Once this is done, we can check that everything behaved correctly with the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hpc/run_1/results/results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the results\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhpc/run_1/results/results.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# As it is, the processing script just returns the input data, so we can simply \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# check that the results are the same as the input data.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/virtual/hpc-templates/NE_3BlhP/hpc-templates/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hpc/run_1/results/results.pkl'"
     ]
    }
   ],
   "source": [
    "# Test the results\n",
    "\n",
    "with open('hpc/run_1/results/results.pkl', 'rb') as f:\n",
    "    results = pkl.load(f)\n",
    "\n",
    "# As it is, the processing script just returns the input data, so we can simply \n",
    "# check that the results are the same as the input data.\n",
    "for i in range(N):\n",
    "    assert np.all(results[i][1] == data[i])\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How large should by job array be? And how many tasks per job?\n",
    "\n",
    "This depends on the cluster you are using and the resources available to you. Here is a function that can help estimate how long a job will take (assuming the entire job array gets started at the same time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_total_time(num_runs, single_run_time, job_array_size, n_tasks_per_job, safety_factor=1.0):\n",
    "    \"\"\"Estimates the amount of time a job will take.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_runs : int\n",
    "        Number of independent runs.\n",
    "    single_run_time : float\n",
    "        Time (in seconds) for a single run.\n",
    "    job_array_size : int\n",
    "        Size of the job array\n",
    "    n_tasks_per_job : int\n",
    "        Number of tasks per job.\n",
    "    safety_factor : float\n",
    "    \"\"\"\n",
    "    total_time = num_runs*single_run_time/(job_array_size*n_tasks_per_job)*safety_factor\n",
    "    hours = total_time//3600\n",
    "    minutes = (total_time - hours*3600)//60\n",
    "    seconds = total_time - hours*3600 - minutes*60\n",
    "    print(f'{hours:.0f} hours, {minutes:.0f} minutes, {seconds:.0f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for instance, if I wanted to do 1,000,000 runs where each run takes 3 minutes, and I want to submit a job array of size 400 and with 50 tasks per subjob, I would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hours, 30 minutes, 0 seconds\n"
     ]
    }
   ],
   "source": [
    "estimate_total_time(\n",
    "    1000000,  # number of runs\n",
    "    180,  # time per run (in seconds)\n",
    "    400,  # job array size\n",
    "    50  # number of tasks per job\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-2024-01-31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
